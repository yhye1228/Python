{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-09T13:03:56.689270Z",
     "start_time": "2025-11-09T13:03:23.139648Z"
    }
   },
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# 데이터 불러오기\n",
    "my_images = []\n",
    "labels = [0]*5 + [1]*10  # 타인=0, 내얼굴=1\n",
    "\n",
    "for i in range(15):\n",
    "    file = f\"./my_images/img{i + 1:02d}.jpg\"\n",
    "    image = cv.imread(file)\n",
    "    image = cv.resize(image, (96, 96))\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    my_images.append(image)\n",
    "\n",
    "X = np.array(my_images, dtype='float32') / 255.0\n",
    "y = np.array(labels)\n",
    "\n",
    "# 학습/검증 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 데이터 증강\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# ✅ MobileNetV2 전이 학습 모델 사용\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96,96,3))\n",
    "base_model.trainable = False  # 특징 추출부 고정\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=2),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(\"MY_FACE_MODEL.keras\")\n",
    "\n",
    "# 테스트\n",
    "test_images = []\n",
    "for i in range(10):\n",
    "    file = f\"./test_images/img{i + 1:02d}.jpg\"\n",
    "    image = cv.imread(file)\n",
    "    image = cv.resize(image, (96, 96))\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    test_images.append(image)\n",
    "\n",
    "X_test = np.array(test_images, dtype='float32') / 255.0\n",
    "cnn_model = keras.models.load_model(\"MY_FACE_MODEL.keras\")\n",
    "\n",
    "predictions = cnn_model.predict(X_test)\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    label = \"✅ 내 얼굴\" if p > 0.5 else \"❌ 타인\"\n",
    "    print(f\"img{i+1:02d}.jpg → {label} ({p[0]:.3f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 393ms/step - accuracy: 0.5000 - loss: 0.7147 - val_accuracy: 0.6000 - val_loss: 0.6874\n",
      "Epoch 2/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step - accuracy: 0.5000 - loss: 0.7852 - val_accuracy: 0.6000 - val_loss: 0.6927\n",
      "Epoch 3/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step - accuracy: 0.5000 - loss: 0.7477 - val_accuracy: 0.6000 - val_loss: 0.6973\n",
      "Epoch 4/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 124ms/step - accuracy: 0.6000 - loss: 0.7722 - val_accuracy: 0.6000 - val_loss: 0.6983\n",
      "Epoch 5/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 129ms/step - accuracy: 0.6000 - loss: 0.6282 - val_accuracy: 0.6000 - val_loss: 0.6999\n",
      "Epoch 6/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 137ms/step - accuracy: 0.8000 - loss: 0.5610 - val_accuracy: 0.6000 - val_loss: 0.7052\n",
      "Epoch 7/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 135ms/step - accuracy: 0.7000 - loss: 0.5357 - val_accuracy: 0.6000 - val_loss: 0.7126\n",
      "Epoch 8/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 139ms/step - accuracy: 0.8000 - loss: 0.7012 - val_accuracy: 0.6000 - val_loss: 0.7179\n",
      "Epoch 9/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 135ms/step - accuracy: 0.8000 - loss: 0.6038 - val_accuracy: 0.6000 - val_loss: 0.7187\n",
      "Epoch 10/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 233ms/step - accuracy: 0.8000 - loss: 0.5027 - val_accuracy: 0.6000 - val_loss: 0.7219\n",
      "Epoch 11/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 137ms/step - accuracy: 0.7000 - loss: 0.6114 - val_accuracy: 0.6000 - val_loss: 0.7258\n",
      "Epoch 12/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 131ms/step - accuracy: 0.6000 - loss: 0.7493 - val_accuracy: 0.6000 - val_loss: 0.7288\n",
      "Epoch 13/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 152ms/step - accuracy: 0.8000 - loss: 0.4949 - val_accuracy: 0.6000 - val_loss: 0.7352\n",
      "Epoch 14/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 140ms/step - accuracy: 0.7000 - loss: 0.5274 - val_accuracy: 0.6000 - val_loss: 0.7395\n",
      "Epoch 15/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 120ms/step - accuracy: 0.8000 - loss: 0.5917 - val_accuracy: 0.6000 - val_loss: 0.7428\n",
      "Epoch 16/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 131ms/step - accuracy: 0.7000 - loss: 0.5902 - val_accuracy: 0.6000 - val_loss: 0.7466\n",
      "Epoch 17/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 121ms/step - accuracy: 0.7000 - loss: 0.5940 - val_accuracy: 0.6000 - val_loss: 0.7530\n",
      "Epoch 18/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 146ms/step - accuracy: 0.8000 - loss: 0.7300 - val_accuracy: 0.6000 - val_loss: 0.7646\n",
      "Epoch 19/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 127ms/step - accuracy: 0.5000 - loss: 0.7282 - val_accuracy: 0.6000 - val_loss: 0.7748\n",
      "Epoch 20/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 142ms/step - accuracy: 0.6000 - loss: 0.7276 - val_accuracy: 0.6000 - val_loss: 0.7790\n",
      "Epoch 21/200\n",
      "\u001B[1m5/5\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 137ms/step - accuracy: 0.7000 - loss: 0.6174 - val_accuracy: 0.6000 - val_loss: 0.7792\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "img01.jpg → ✅ 내 얼굴 (0.814)\n",
      "img02.jpg → ✅ 내 얼굴 (0.756)\n",
      "img03.jpg → ✅ 내 얼굴 (0.534)\n",
      "img04.jpg → ✅ 내 얼굴 (0.699)\n",
      "img05.jpg → ✅ 내 얼굴 (0.743)\n",
      "img06.jpg → ✅ 내 얼굴 (0.807)\n",
      "img07.jpg → ✅ 내 얼굴 (0.767)\n",
      "img08.jpg → ✅ 내 얼굴 (0.742)\n",
      "img09.jpg → ✅ 내 얼굴 (0.743)\n",
      "img10.jpg → ✅ 내 얼굴 (0.792)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T13:03:56.709891Z",
     "start_time": "2025-11-09T13:03:56.703059Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e4b2b27db0ecc512",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
